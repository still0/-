{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'股票名称': '东方财富', '今开': '13.10', '成交量': '130.12万手', '最高': '13.17', '涨停': '14.52', '内盘': '66.28万手', '成交额': '16.79亿', '委比': '44.54%', '流通市值': '532.06亿', '市盈率MRQ': '62.00', '每股收益': '0.15', '总股本': '51.68亿', '昨收': '13.20', '换手率': '3.13%', '最低': '12.58', '跌停': '\\n                        11.88', '外盘': '65.86万手', '振幅': '4.47%', '量比': '3.02', '总市值': '662.05亿', '市净率': '4.63', '每股净资产': '2.77', '流通股本': '41.53亿', '主力流入': '--', '散户流入': '--', '主力流出': '--', '散户流出': '--'}\n",
      "当前进度：0.02%获取失败\n",
      "获取股票信息失败\n",
      "当前进度：0.04%获取失败\n",
      "获取股票信息失败\n",
      "当前进度：0.06%{'股票名称': 'R014', '主力流入': '--', '散户流入': '--', '主力流出': '--', '散户流出': '--'}\n",
      "当前进度：0.08%"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "import traceback\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def getHtmltxt(url,code = 'utf-8'):\n",
    "    try:\n",
    "        kv = {'user-agent':'Mozilla/5.0'}\n",
    "        r = rq.get(url,timeout = 30,headers = kv)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = code\n",
    "        return r.text\n",
    "    except:\n",
    "        print('获取失败')\n",
    "        \n",
    "def getstocklist(lst,stockurl,outlist):\n",
    "    try:\n",
    "        html = getHtmltxt(stockurl,'GB2312')        \n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        a = soup.find_all('a')\n",
    "        for i in a :\n",
    "            try :\n",
    "                href = i.attrs['href']\n",
    "                lst.append(re.findall(r'[s][hz]\\d{6}', href)[0])\n",
    "                \n",
    "            except :\n",
    "                continue\n",
    "        with open(outlist,'a',encoding = 'utf-8') as f :\n",
    "            \n",
    "            f.write(str(lst)+'\\n')\n",
    "            f.close()\n",
    "                       \n",
    "    except:\n",
    "        print('获取股票信息列表失败')\n",
    "        \n",
    "                \n",
    "        \n",
    "def getstockinfo(lst,stockurl,outpath):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for stock in lst:\n",
    "        url = stockurl + stock + '.html'\n",
    "        html = getHtmltxt(url)\n",
    "        \n",
    "        try :\n",
    "            \n",
    "            if html == '':\n",
    "                continue\n",
    "            \n",
    "            infodict = {}\n",
    "            soup = BeautifulSoup(html,'html.parser')\n",
    "            stockinfo = soup.find('div',attrs = {'class':'stock-bets'})\n",
    "            \n",
    "            name = stockinfo.find_all(attrs = {'class':'bets-name'})[0]\n",
    "            \n",
    "            infodict.update({'股票名称': name.text.split(' ')[12]})\n",
    "            \n",
    "            keylist = soup.find_all('dt')\n",
    "            \n",
    "            valuelist = soup.find_all('dd')\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(len(keylist)):\n",
    "                key = keylist[i].text\n",
    "                value = valuelist[i].text\n",
    "                infodict[key] = value\n",
    "            print(infodict)\n",
    "            \n",
    "            with open(outpath,'a',encoding = 'utf-8') as g :\n",
    "                g.write(str(infodict)+'\\n')\n",
    "                       \n",
    "            count+=1\n",
    "            print('\\r当前进度：{:.2f}%'.format(count*100/len(lst)),end='')\n",
    "            \n",
    "        except :\n",
    "            print('获取股票信息失败')\n",
    "            count+=1\n",
    "            print('\\r当前进度：{:.2f}%'.format(count*100/len(lst)),end='')\n",
    "            continue\n",
    "           \n",
    "              \n",
    "    \n",
    "\n",
    "def main():\n",
    "    stock_list_url = 'http://quote.eastmoney.com/stocklist.html'\n",
    "    stock_info_url = 'https://gupiao.baidu.com/stock/'\n",
    "    outlist = 'E:\\stocklist.txt'\n",
    "    outpath = 'E:\\stockinfo.txt'\n",
    "    \n",
    "    slist = []\n",
    "    getstocklist(slist,stock_list_url,outlist)\n",
    "    getstockinfo(slist,stock_info_url,outpath)\n",
    "          \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list写入\n",
    "with open('E:\\stocklist.txt','a',encoding = 'utf-8') as f :\n",
    "            \n",
    "            f.write(str(lst)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import RequestException\n",
    "from pyquery import PyQuery as pq\n",
    "import time\n",
    "import csv\n",
    "from multiprocessing.pool import Pool\n",
    "from threading import Thread\n",
    "import random\n",
    "import requests as rq\n",
    "import traceback\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def getHtmltxt(url,code = 'utf-8'):\n",
    "    try:\n",
    "        kv = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "        r = rq.get(url,timeout = 5,headers = kv)\n",
    "        if r.status_code == 200:\n",
    "            r.encoding = code\n",
    "            return r.text            \n",
    "        \n",
    "    except:\n",
    "        print('访问失败')\n",
    "        \n",
    "def getstocklist(lst,stockurl,outlist):\n",
    "    try:\n",
    "        html = getHtmltxt(stockurl,'GB2312')        \n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        ul= soup.find_all('a')\n",
    "        for i in ul :\n",
    "            try :\n",
    "                \n",
    "                href = i.attrs['href']\n",
    "                lst.append(re.findall(r'[s][hz]\\d{6}', href)[0])\n",
    "                \n",
    "            except :\n",
    "                continue\n",
    "        with open(outlist,'a',encoding = 'utf-8') as f :\n",
    "            \n",
    "            f.write(str(lst)+'\\n')\n",
    "            \n",
    "        with open('outlist.csv','r',newline = '') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for i in lst:\n",
    "                writer.writerrow([i])\n",
    "            \n",
    "                       \n",
    "    except:\n",
    "        print('获取股票信息列表失败')\n",
    "        \n",
    "                \n",
    "        \n",
    "def getstockinfo(lst,stockurl,outpath):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(lst)):\n",
    "        url = stockurl + lst[i] + '.html'\n",
    "        html = getHtmltxt(url)\n",
    "        \n",
    "        try :\n",
    "            \n",
    "            if html == '':\n",
    "                continue\n",
    "            \n",
    "            infodict = {}\n",
    "            soup = BeautifulSoup(html,'lxml')\n",
    "            stockinfo = soup.find('div',attrs = {'class':'stock-bets'})\n",
    "            \n",
    "            name = stockinfo.find_all(attrs = {'class':'bets-name'})[0]\n",
    "            \n",
    "            infodict.update({'股票名称': name.text.split(' ')[12]})\n",
    "            \n",
    "            keylist = soup.find_all('dt')\n",
    "            \n",
    "            valuelist = soup.find_all('dd')\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(len(keylist)):\n",
    "                key = keylist[i].text\n",
    "                value = valuelist[i].text\n",
    "                infodict[key] = value\n",
    "            print(infodict)\n",
    "            \n",
    "            with open(outpath,'a',encoding = 'utf-8') as g :\n",
    "                g.write(str(infodict)+'\\n')\n",
    "                       \n",
    "            count+=1\n",
    "            print('\\r当前进度：{:.2f}%'.format(count*100/len(lst)),end='')\n",
    "            \n",
    "        except :\n",
    "            print('获取股票信息失败')\n",
    "            count+=1\n",
    "            print('\\r当前进度：{:.2f}%'.format(count*100/len(lst)),end='')\n",
    "            continue\n",
    "           \n",
    "              \n",
    "    \n",
    "\n",
    "def main():\n",
    "    stock_list_url = 'http://quote.eastmoney.com/stocklist.html'\n",
    "    stock_info_url = 'https://gupiao.baidu.com/stock/'\n",
    "    outlist = 'E:\\stocklist.txt'\n",
    "    outpath = 'E:\\stockinfo.txt'\n",
    "    \n",
    "    slist = []\n",
    "    getstocklist(slist,stock_list_url,outlist)\n",
    "    getstockinfo(slist,stock_info_url,outpath)\n",
    "          \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "import time\n",
    "import csv\n",
    "import threading \n",
    "import random\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "class stock():\n",
    "    \"\"\"\n",
    "    获取股票信息\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.listurl =  'http://quote.eastmoney.com/stocklist.html'\n",
    "        self.stockurl = 'https://gupiao.baidu.com/stock/'\n",
    "        self.list = []\n",
    "        self.infodict = {}\n",
    "        self.threadmax = threading.BoundedSemaphore(20)\n",
    "        \n",
    "    def gettext(self,url,code = 'utf-8'):\n",
    "        \"\"\"\n",
    "        获取网页函数，返回网页text\n",
    "        \n",
    "        \"\"\"\n",
    "        IP = '121.17.18.218'\n",
    "        PORT = '8060'\n",
    "        proxy = IP + \":\" + PORT\n",
    "        proxies = {'http': 'http://' + proxy,'https': 'https://' + proxy,}\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "        response = requests.get(url,headers = self.headers,timeout = 5,verify = False) #,,proxies = proxies\n",
    "        try :\n",
    "            if response.status_code == 200:\n",
    "                response.encoding = code\n",
    "                return response.text\n",
    "            \n",
    "        except :\n",
    "            print('获取网页失败')\n",
    "            return None\n",
    "    \n",
    "    def getlist(self,text):\n",
    "        try:\n",
    "            \n",
    "            doc = pq(text)\n",
    "            \n",
    "            li = doc('#quotesearch ul li')\n",
    "            \n",
    "            for i in li.items():\n",
    "                num = ''\n",
    "                num = li('a').attr('href').split('.')[-2]\n",
    "                print(num)\n",
    "                if num :\n",
    "                    self.list.append(num)\n",
    "        except:\n",
    "            print('获取股票信息列表失败')\n",
    "            \n",
    "    def main(self):\n",
    "        listtext = self.gettext(self.listurl,'GBK')\n",
    "        self.getlist(listtext)\n",
    "        print(self.list)\n",
    "        \n",
    "if __name__ == '__main__': \n",
    "    stock1 = stock()    \n",
    "    stock1.main()\n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
